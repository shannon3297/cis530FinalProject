{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import string\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_feats = pd.read_csv('train-dev-test/train_cleaned_features.csv', delimiter=',', encoding=\"utf-8\")\n",
    "dev_feats = pd.read_csv('train-dev-test/dev_cleaned_features.csv', delimiter=',', encoding=\"utf-8\")\n",
    "test_feats = pd.read_csv('train-dev-test/test_cleaned_features.csv', delimiter=',', encoding=\"utf-8\")\n",
    "\n",
    "# Clear whitespace\n",
    "train_feats.columns = train_feats.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length:  3515562\n",
      "New length:  3515538\n"
     ]
    }
   ],
   "source": [
    "# Dropping stuff!\n",
    "\n",
    "# Drop all rows where time is NA?\n",
    "print(\"Old length: \", len(train_feats))\n",
    "train_feats = train_feats[train_feats['time'].notna()]\n",
    "print(\"New length: \", len(train_feats))\n",
    "\n",
    "# Just drop hashtags for now, too difficult for M3 to deal with\n",
    "train_feats = train_feats.drop(['hashtags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>place</th>\n",
       "      <th>user_location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>num_present</th>\n",
       "      <th>trump_present</th>\n",
       "      <th>hashtag_present</th>\n",
       "      <th>covid</th>\n",
       "      <th>vaccine</th>\n",
       "      <th>profanity_present</th>\n",
       "      <th>emoji_present</th>\n",
       "      <th>url_present</th>\n",
       "      <th>question_exclamation_present</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretty much any corona virus germ that wants t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PG County</td>\n",
       "      <td>478</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Mar</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19:52:15</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @RealSaavedra: Good.\\n\\nIt came from China.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sydney, New South Wales</td>\n",
       "      <td>394</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Mar</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19:52:15</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @nicolebyer: Everyone in this looks sick</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ferrets</td>\n",
       "      <td>695</td>\n",
       "      <td>48499</td>\n",
       "      <td>0</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Mar</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19:52:15</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jabinbotsford: Close up of President @real...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>30492</td>\n",
       "      <td>0</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Mar</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19:52:15</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Biancaixvi: Corona day 3: it just feels li...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Huntley, IL</td>\n",
       "      <td>392</td>\n",
       "      <td>85302</td>\n",
       "      <td>0</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Mar</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19:52:15</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text place  \\\n",
       "0  Pretty much any corona virus germ that wants t...   NaN   \n",
       "1     RT @RealSaavedra: Good.\\n\\nIt came from China.   NaN   \n",
       "2        RT @nicolebyer: Everyone in this looks sick   NaN   \n",
       "3  RT @jabinbotsford: Close up of President @real...   NaN   \n",
       "4  RT @Biancaixvi: Corona day 3: it just feels li...   NaN   \n",
       "\n",
       "             user_location followers_count retweet_count favorite_count  \\\n",
       "0                PG County             478             0             10   \n",
       "1  Sydney, New South Wales             394           102              0   \n",
       "2                  ferrets             695         48499              0   \n",
       "3                      NaN              19         30492              0   \n",
       "4              Huntley, IL             392         85302              0   \n",
       "\n",
       "  weekday month   day      time  ...  num_present  trump_present  \\\n",
       "0     Thu   Mar  19.0  19:52:15  ...        False          False   \n",
       "1     Thu   Mar  19.0  19:52:15  ...        False          False   \n",
       "2     Thu   Mar  19.0  19:52:15  ...        False          False   \n",
       "3     Thu   Mar  19.0  19:52:15  ...        False           True   \n",
       "4     Thu   Mar  19.0  19:52:15  ...         True          False   \n",
       "\n",
       "   hashtag_present  covid  vaccine  profanity_present  emoji_present  \\\n",
       "0            False   True    False              False          False   \n",
       "1            False  False    False              False          False   \n",
       "2            False  False    False              False          False   \n",
       "3            False   True    False              False          False   \n",
       "4            False   True    False              False          False   \n",
       "\n",
       "   url_present  question_exclamation_present  sentiment  \n",
       "0         True                         False   positive  \n",
       "1        False                         False   positive  \n",
       "2        False                         False   negative  \n",
       "3        False                         False    neutral  \n",
       "4        False                         False    neutral  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataframes\n",
    "train_feats.head(5)\n",
    "#train_feats.columns                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                             object\n",
       "place                            object\n",
       "user_location                    object\n",
       "followers_count                  object\n",
       "retweet_count                    object\n",
       "favorite_count                   object\n",
       "weekday                          object\n",
       "month                            object\n",
       "day                             float64\n",
       "time                             object\n",
       "year                            float64\n",
       "num_present                        bool\n",
       "trump_present                      bool\n",
       "hashtag_present                    bool\n",
       "covid                              bool\n",
       "vaccine                            bool\n",
       "profanity_present                  bool\n",
       "emoji_present                      bool\n",
       "url_present                        bool\n",
       "question_exclamation_present       bool\n",
       "sentiment                        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature manipulation to make them ready for the model\n",
    "# Change sentiment labels to numbers!\n",
    "train_feats['sentiment'] = train_feats['sentiment'].astype('category')\n",
    "train_feats[\"sentiment\"] = train_feats[\"sentiment\"].cat.codes\n",
    "\n",
    "# Change location\n",
    "train_feats[\"user_location\"] = train_feats[\"user_location\"].astype('category')\n",
    "train_feats[\"user_location\"] = train_feats[\"user_location\"].cat.codes\n",
    "\n",
    "# Change place\n",
    "train_feats[\"place\"] = train_feats[\"place\"].astype('category')\n",
    "train_feats[\"place\"] = train_feats[\"place\"].cat.codes\n",
    "\n",
    "# Change weekday\n",
    "#train_feats[\"weekday\"] = train_feats[\"weekday\"].astype('category')\n",
    "#train_feats[\"weekday\"] = train_feats[\"weekday\"].cat.codes\n",
    "\n",
    "# Change month\n",
    "train_feats[\"month\"] = train_feats[\"month\"].astype('category')\n",
    "train_feats[\"month\"] = train_feats[\"month\"].cat.codes\n",
    "\n",
    "# Change time to datetime object\n",
    "#train_feats['time'] = train_feats['time'].apply(lambda x: datetime.datetime.strptime(str(x), '%H:%M:%S'))\n",
    "\n",
    "# Add hour and minute columns\n",
    "#train_feats['hour'] = train_feats['time'].apply(lambda x: x.hour)\n",
    "#train_feats['minute'] = train_feats['time'].apply(lambda x: x.minute)\n",
    "\n",
    "# Drop time\n",
    "train_feats = train_feats.drop(['time'], axis=1)\n",
    "\n",
    "# Change all the bool types to numeric\n",
    "train_feats[\"num_present\"] = train_feats[\"num_present\"].astype(int)\n",
    "train_feats[\"trump_present\"] = train_feats[\"trump_present\"].astype(int)\n",
    "#train_feats[\"hashtag_present\"] = train_feats[\"hashtag_present\"].astype(int)\n",
    "train_feats[\"covid\"] = train_feats[\"covid\"].astype(int)\n",
    "train_feats[\"vaccine\"] = train_feats[\"vaccine\"].astype(int)\n",
    "train_feats[\"profanity_present\"] = train_feats[\"profanity_present\"].astype(int)\n",
    "#train_feats[\"emoji_present\"] = train_feats[\"emoji_present\"].astype(int)\n",
    "train_feats[\"url_present\"] = train_feats[\"url_present\"].astype(int)\n",
    "#train_feats[\"question_exclamation_present\"] = train_feats[\"question_exclamation_present\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>place</th>\n",
       "      <th>user_location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>num_present</th>\n",
       "      <th>trump_present</th>\n",
       "      <th>hashtag_present</th>\n",
       "      <th>covid</th>\n",
       "      <th>vaccine</th>\n",
       "      <th>profanity_present</th>\n",
       "      <th>emoji_present</th>\n",
       "      <th>url_present</th>\n",
       "      <th>question_exclamation_present</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretty much any corona virus germ that wants t...</td>\n",
       "      <td>-1</td>\n",
       "      <td>26640</td>\n",
       "      <td>478</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Thu</td>\n",
       "      <td>6</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @RealSaavedra: Good.\\n\\nIt came from China.</td>\n",
       "      <td>-1</td>\n",
       "      <td>33359</td>\n",
       "      <td>394</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>Thu</td>\n",
       "      <td>6</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  place  user_location  \\\n",
       "0  Pretty much any corona virus germ that wants t...     -1          26640   \n",
       "1     RT @RealSaavedra: Good.\\n\\nIt came from China.     -1          33359   \n",
       "\n",
       "  followers_count retweet_count favorite_count weekday  month   day    year  \\\n",
       "0             478             0             10     Thu      6  19.0  2020.0   \n",
       "1             394           102              0     Thu      6  19.0  2020.0   \n",
       "\n",
       "   num_present  trump_present  hashtag_present  covid  vaccine  \\\n",
       "0            0              0            False      1        0   \n",
       "1            0              0            False      0        0   \n",
       "\n",
       "   profanity_present  emoji_present  url_present  \\\n",
       "0                  0          False            1   \n",
       "1                  0          False            0   \n",
       "\n",
       "   question_exclamation_present  sentiment  \n",
       "0                         False          2  \n",
       "1                         False          2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Unsupervised stuff in this cell!\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    pretty much any corona virus germ that wants t...\n",
       "1         rt @realsaavedra: good\\n\\nit came from china\n",
       "2          rt @nicolebyer: everyone in this looks sick\n",
       "3    rt @jabinbotsford: close up of president @real...\n",
       "4    rt @biancaixvi: corona day 3: it just feels li...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "# Remove punctuation\n",
    "train_feats['text'] = \\\n",
    "train_feats['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Convert the titles to lowercase\n",
    "train_feats['text'] = \\\n",
    "train_feats['text'].map(lambda x: x.lower())\n",
    "# Print out the first rows of papers\n",
    "train_feats['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(train_feats['text'].values))\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/varunramakrishnan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pretty', 'much', 'corona', 'virus', 'germ', 'wants', 'entered', 'body', 'passed', 'hours', 'bodied', 'shots', 'henny', 'https', 'tco', 'hlssenrksu']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "data = train_feats.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.068*\"rt\" + 0.057*\"white\" + 0.048*\"fat\" + 0.043*\"big\" + 0.041*\"corona\" + '\n",
      "  '0.037*\"took\" + 0.030*\"house\" + 0.029*\"bitch\" + 0.025*\"schedule\" + '\n",
      "  '0.024*\"smelling\"'),\n",
      " (1,\n",
      "  '0.075*\"rt\" + 0.055*\"corona\" + 0.042*\"virus\" + 0.036*\"realdonaldtrump\" + '\n",
      "  '0.035*\"chinese\" + 0.035*\"president\" + 0.032*\"every\" + 0.031*\"shit\" + '\n",
      "  '0.026*\"crossed\" + 0.025*\"close\"'),\n",
      " (2,\n",
      "  '0.097*\"rt\" + 0.083*\"millennials\" + 0.071*\"want\" + 0.056*\"love\" + '\n",
      "  '0.049*\"name\" + 0.048*\"folks\" + 0.048*\"calling\" + 0.045*\"everybody\" + '\n",
      "  '0.044*\"generation\" + 0.042*\"corona\"'),\n",
      " (3,\n",
      "  '0.061*\"rt\" + 0.038*\"corona\" + 0.036*\"get\" + 0.015*\"family\" + 0.014*\"still\" '\n",
      "  '+ 0.013*\"trying\" + 0.012*\"let\" + 0.011*\"gonna\" + 0.010*\"day\" + 0.010*\"end\"'),\n",
      " (4,\n",
      "  '0.102*\"rt\" + 0.067*\"corona\" + 0.041*\"could\" + 0.040*\"everyone\" + '\n",
      "  '0.036*\"man\" + 0.036*\"sick\" + 0.034*\"stop\" + 0.021*\"piss\" + '\n",
      "  '0.021*\"biondiewasabi\" + 0.020*\"looks\"'),\n",
      " (5,\n",
      "  '0.083*\"rt\" + 0.020*\"corona\" + 0.019*\"trump\" + 0.014*\"racist\" + '\n",
      "  '0.013*\"coronavirus\" + 0.012*\"like\" + 0.011*\"virus\" + 0.009*\"using\" + '\n",
      "  '0.009*\"else\" + 0.007*\"incompetent\"'),\n",
      " (6,\n",
      "  '0.050*\"rt\" + 0.037*\"covid\" + 0.018*\"people\" + 0.012*\"one\" + 0.010*\"trump\" + '\n",
      "  '0.008*\"mask\" + 0.007*\"know\" + 0.007*\"would\" + 0.007*\"like\" + 0.007*\"need\"'),\n",
      " (7,\n",
      "  '0.128*\"https\" + 0.126*\"tco\" + 0.041*\"rt\" + 0.031*\"corona\" + 0.018*\"covid\" + '\n",
      "  '0.010*\"virus\" + 0.010*\"vaccine\" + 0.007*\"home\" + 0.007*\"stay\" + '\n",
      "  '0.006*\"got\"'),\n",
      " (8,\n",
      "  '0.065*\"rt\" + 0.040*\"covid\" + 0.020*\"vaccine\" + 0.019*\"corona\" + '\n",
      "  '0.013*\"cases\" + 0.012*\"new\" + 0.010*\"virus\" + 0.008*\"get\" + '\n",
      "  '0.008*\"coronavirus\" + 0.006*\"people\"'),\n",
      " (9,\n",
      "  '0.078*\"rt\" + 0.066*\"pandemic\" + 0.018*\"corona\" + 0.015*\"weeks\" + '\n",
      "  '0.013*\"fucking\" + 0.013*\"global\" + 0.011*\"amp\" + 0.008*\"post\" + '\n",
      "  '0.008*\"first\" + 0.008*\"strain\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_NUMEXPR_INSTALLED' from 'pandas.core.computation.check' (/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/check.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7253ca5f8459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensimvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/gensim_models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\"\n\u001b[1;32m    122\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0mterm_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     topic_info = _topic_info(topic_term_dists, topic_proportion,\n\u001b[0m\u001b[1;32m    440\u001b[0m                              \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                              n_jobs, start_index)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;31m# this determines the R terms that are displayed when no topic is selected.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mtt_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mtopic_given_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topic_term_dists / tt_sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0mlog_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topic_given_term.T / topic_proportion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topic_given_term * log_1.T\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(expr, parser, engine, truediv, local_dict, global_dict, resolvers, level, target, inplace)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;34m\"multi-line expressions are only valid in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;34m\"context of data, use DataFrame.eval\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         )\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0m_check_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/eval.py\u001b[0m in \u001b[0;36m_check_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mEngine\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNUMEXPR_INSTALLED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_NUMEXPR_INSTALLED' from 'pandas.core.computation.check' (/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/check.py)"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import os\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, dictionary=id2word)\n",
    "vis\n",
    "\n",
    "'''\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, dictionary=id2word)\n",
    "vis\n",
    "'''\n",
    "'''\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = train_feats['text'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=100, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "processed_features = vectorizer.fit_transform(all_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop text (JUST FOR A TEST)\n",
    "train_feats = train_feats.drop(['text'], axis=1)\n",
    "train_feats = train_feats.drop(['weekday'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e65356f898c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get data ready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Get sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_feats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Drop labels from larger dataframee\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentiment'"
     ]
    }
   ],
   "source": [
    "# Get data ready\n",
    "# Get sentiment\n",
    "labels = train_feats['sentiment']\n",
    "# Drop labels from larger dataframee\n",
    "train_feats = train_feats.drop(['sentiment'], axis=1)\n",
    "\n",
    "# Convert both to numpy arrays\n",
    "#non_text_feats = train_feats.to_numpy()\n",
    "#X = np.concatenate((processed_features, non_text_feats), axis=1)\n",
    "X = processed_features\n",
    "y = labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model - Multinomial logistic regression\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# Train model\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6274510473219177\n"
     ]
    }
   ],
   "source": [
    "y_preds = model.predict(X)\n",
    "# Print score\n",
    "print(f1_score(y, y_preds, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "db7d3e87e6340b482333b66fc9e1422c5abdf2e2aa02949130528e569bdc2c91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
